# General RL settings
rl:
  buffer_size: 200_000 # Not so important. Decrease this as much as we can.
  td_steps: 7 # Ablated from 1 to 10.
  td_lambda: 0.95 # Ablated from 0.5, 0.75, 0.9, 0.95, 0.975, 0.99, 1.0.
  unroll_steps: 1 # Can increase this if we want. But not very important since we don't do deep search.
  discount: 0.975 # Original value of MuZero
  n_warmup: 150_000 # Keep it this way.
  training_steps: 100_000 # Not so important, keep it this way. Can decrease this if we want faster iteration.
  frame_skip: 2 # Keep it this way.
  target_network_update_freq: 200 # Ablated on 100, 150, 200, 250, 300.
  tau: 0.0005 # Ablated on 0.0005, 0.00075. Training is not instable anyway so we keep it this way.
  soft_update_critic: false 
  soft_update_all: false
  use_second_player_data: false # Does not seem to help.
  PER: true # Not so important. Can be turned off. Not exactly PER but a variant.
  action_masking: false  # Can be turned off. Marginal improvement, if at all.

# Optimization settings
optimizer:
  batch_size: 1024 # Did not experiment much.
  lr: 6e-4 # Original value of MuZero.
  max_avg_gradient_steps_per_frame: 4 # The lower the better. Increase this in development to around 16-32 to speed up development.
  min_avg_gradient_steps_per_frame: 1 # Not really important since we need as much data as we can get.
  value_loss_weight: 0.5 # Did not experiment much.
  reward_loss_weight: 1 # Original value of MuZero.
  dynamic_loss_weight: 1.5 # Did not experiment much.
  scheduler_option: cosine # cosine, cosine_constant_200k, cosine_linear_200k
  schedulers:
    cosine:
      eta_min: 0
      t_max: 100_000
    cosine_constant_200k:
      eta_min: 5.43e-5
      t_max: 80_550
    cosine_linear_200k:
      eta_min: 5.43e-5
      t_max: 80_550

# Network architecture
architecture:
  hidden_dim: 128 # Can be increased further.
  bottle_neck_expansion_dim: 256 # Usually we see factors 4 of hidden_dim. But we keep at 2 for now since many workers are used. Bigger graphic card can fit more workers or bigger expansion dim.
  activation_function: ReLU # Did not experiment around much.
  encoder_num_blocks: 3 # May be can be increased further.
  dynamic_num_blocks: 3 # May be can be increased further.
  critic_expand_factor: 1.5 # If we can not scale the whole network, we can at least scale the width of the state-value network.
  num_experts: 2 # The higher the better, tested until 4.
  running_stats_norm: true # Could be turned off.

# Ray settings
ray:
  num_envs_per_worker: 16 # Keep it this way.
  shared_network_update_freq: 15 # Keep it this way.
  shared_network_load_worker_freq: 5 # Keep it this way.
  logging_freq: 50 # Not important.
  max_memory: 2500 # MBs. Forecasting has some weird memory leak. We kill a worker if its memory consumption exceeds this limit. Set this to lower value if we have little RAM.
  epsilons: [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.15, 0.05] # Add as many values as we to increase #workers, #CPU is a bottleneck. Each is the epsilon of the epsilon-greedy policy of a worker.

# Evaluation setting:
evaluation:
  start_evaluation: 500 # training steps
  frame_skip: 1 # Keep it this way.
  n_games: 100 # Can increase further, but too much will cause OOM since we do batch evaluation.
  evaluation_interval: 300 # seconds. Decrease this will increase chance of good checkpoints. But slower training. 

# Environment setting
env:
  use_forecast: true
  forecast_step: 5
  obs_dim: 18
  action_dim: 4
  obs_augmentation: true
  obs_augmentation_dim: 27
  obs_mean: [-1.9823250252696023, 0.01130789504406784, -0.04614589924628937, 0.5673360873132196, 0.023351425438805258, -0.00021966681879202778, 1.9744109851511855, -0.0107219188224735, -0.04169986524483901, -0.5721948952876487, -0.016573759707919574, -0.0005931500541237432, -0.0020039421461280657, -0.0016634215100545622, -0.0032928255735346014, 0.01050138346874063, 1.4101711281071116, 1.4282973231358218, 4.06802451004443, 2.5628726825945805, 2.5552315508248875, 4.972713418570301, 11.873044863222123, 4.278629860929123, 8.28363243406036, 4.531650738126346, 10.150353076184224]
  obs_std: [0.9756473939149155, 0.831435894303476, 0.6141568712936554, 4.591658887791821, 3.9336834937080263, 5.811646728069116, 0.978356828605632, 0.8363213812716808, 0.6149842161140934, 4.5867035081271155, 3.942118739456129, 5.7991076591158945, 1.860142773433321, 1.6441191814933296, 17.870246262804233, 9.772248174663309, 3.70219423537114, 3.721688292451709, 1.091043191110036, 1.6049245913743584, 1.6063594948091153, 0.8676926816207443, 0.9400313230983509, 0.859005057781339, 0.923298740323576, 1.529213998904212, 1.7920433865408545]
  forecast_mean: [-0.0020039421461280657, -0.0016634215100545622,]
  forecast_std: [1.860142773433321, 1.6441191814933296]
  action_space:  [
    [0, 0, 0, 1],
    [0, 1, 1, 0],
    [1, 1, 1, 0],
    [1, 0, 1, 0],
    [1, -1, 1, 0],
    [0, -1, 1, 0],
    [-1, -1, 1, 0],
    [-1, 0, 1, 0],
    [-1, 1, 1, 0],
    [0, 1, -1, 0],
    [1, 1, -1, 0],
    [1, 0, -1, 0],
    [1, -1, -1, 0],
    [0, -1, -1, 0],
    [-1, -1, -1, 0],
    [-1, 0, -1, 0],
    [-1, 1, -1, 0],
    [0, 1, 0, 0],
    [1, 1, 0, 0],
    [1, 0, 0, 0],
    [1, -1, 0, 0],
    [0, -1, 0, 0],
    [-1, -1, 0, 0],
    [-1, 0, 0, 0],
    [-1, 1, 0, 0]]
